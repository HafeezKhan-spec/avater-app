import React, { useState, useRef } from "react";
import { Canvas, useFrame } from "@react-three/fiber";
import { OrbitControls, useGLTF } from "@react-three/drei";

function AvatarWithLipSync({ lipSyncData, audioRef }) {
  const { scene } = useGLTF("/models/casual_man.glb");
  const meshRef = useRef();
  useFrame(() => {
    if (!audioRef.current || !meshRef.current) return;
    const currentTime = audioRef.current.currentTime;

    if (meshRef.current.morphTargetInfluences) {
      for (let i = 0; i < meshRef.current.morphTargetInfluences.length; i++) {
        meshRef.current.morphTargetInfluences[i] = 0;
      }
    }

    const currentCue = lipSyncData.find(
      (cue) => currentTime >= cue.start && currentTime < cue.end
    );

    if (currentCue && meshRef.current.morphTargetDictionary) {
      const index = meshRef.current.morphTargetDictionary[currentCue.morphTargetName];
      if (index !== undefined) {
        meshRef.current.morphTargetInfluences[index] = 1;
      }
    }
  });

  return (
    <primitive
      ref={meshRef}
      object={scene}
      position={[1.1, -6.7, 0]}
      rotation={[0, 0, 0]}
      scale={4}
      dispose={null}
    />
  );
}

function SpeechTextBox({ text }) {
  return (
    <div
      style={{
        position: "absolute",
        left: 40,
        top: "22%",
        width: "28vw",
        height: "fit-content",
        background: "rgba(255,255,255,0.18)",
        color: "#21334a",
        borderRadius: "18px",
        boxShadow: "0 2px 14px rgba(0,0,0,0.04)",
        fontSize: "1.6rem",
        padding: "2rem",
        zIndex: 2,
        pointerEvents: "none"
      }}
    >
      {text}
    </div>
  );
}

export default function App() {
  const [speechText, setSpeechText] = useState("Hello, how can I help?");
  const audioRef = useRef();

  // Example lip sync data for demonstration
  const lipSyncData = [
    { start: 0, end: 0.5, morphTargetName: "mouthOpen" },
    { start: 0.5, end: 1.0, morphTargetName: "mouthSmile" },
    { start: 1.0, end: 1.5, morphTargetName: "mouthOpen" },
    { start: 1.5, end: 2.0, morphTargetName: "mouthSmile" },
  ];

  // Integrate with your backend as before
  async function handleLLMInference(userText) {
    // Fetch or generate text, TTS url, and lip sync data from your model and backend here

    const data = {
      text: "This was generated by your LLM.",
      ttsAudioUrl: "/path-to-generated-audio.mp3",
      lipSyncData: lipSyncData
    };

    setSpeechText(data.text);
    audioRef.current.src = data.ttsAudioUrl;
    audioRef.current.play();
    // Update lipSyncData if dynamic
    // setLipSyncData(data.lipSyncData);
  }

  return (
    <div style={{ width: "100vw", height: "100vh", position: "relative" }}>
      <SpeechTextBox text={speechText} />
      <audio ref={audioRef} src="/path-to-your-tts-audio.mp3" style={{ display: "none" }} autoPlay />
      {/* Example button to trigger integration */}
      {/* <button onClick={() => handleLLMInference("Hello")}>Ask</button> */}
      <Canvas
        style={{ background: "linear-gradient(135deg, #43cea2 0%, #185a9d 100%)" }}
        camera={{ position: [0.75, 1.1, 2.75], fov: 38 }}
      >
        <ambientLight intensity={0.9} />
        <directionalLight position={[5, 10, 5]} intensity={1} />
        <AvatarWithLipSync lipSyncData={lipSyncData} audioRef={audioRef} />
        <OrbitControls
          enablePan={false}
          enableZoom={false}
          enableRotate={false}
          minPolarAngle={Math.PI / 2.3}
          maxPolarAngle={Math.PI / 2.5}
        />
      </Canvas>
    </div>
  );
}
